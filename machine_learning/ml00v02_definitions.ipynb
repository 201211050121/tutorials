{"nbformat": 3, "nbformat_minor": 0, "worksheets": [{"cells": [{"source": ["Definitions and Advices"], "cell_type": "heading", "metadata": {}, "level": 1}, {"source": ["* [1 What is Machine Learning ?](#1-What-is-Machine-Learning-?)\n", "    * [1.1 Documentation and reference:](#1.1-Documentation-and-reference:)\n", "* [2 Supervised and Unsupervised Learning](#2-Supervised-and-Unsupervised-Learning)\n", "* [3 Cheat Sheet - scikit-learn Algorithm Selection](#3-Cheat-Sheet---scikit-learn-Algorithm-Selection)\n", "* [4 Machine Learning Wisdom](#4-Machine-Learning-Wisdom)\n"], "cell_type": "markdown", "metadata": {}}, {"cell_type": "code", "language": "python", "outputs": [{"output_type": "pyout", "html": ["<style>\n", "    @font-face {\n", "        font-family: \"Computer Modern\";\n", "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n", "    }\n", "    div.cell{\n", "        width:900px;\n", "        margin-left:0% !important;\n", "        margin-right:0%;\n", "    }\n", "\n", "\tcode{\n", "\t\tfont-size:10pt\n", "\t}\n", "\n", "    h1 {\n", "        font-family: Tahoma, sans-serif;\n", "\t\tcolor: rgb( 10, 88, 126 );\n", "\t\tfont-size:28pt;\n", "    }\n", "\th2 {\n", "        font-family: Tahoma, sans-serif;\n", "\t\tcolor: rgb( 10, 88, 126 );\n", "\t\tfont-size:24pt;\n", "    }\n", "\th3 {\n", "        font-family: Tahoma, sans-serif;\n", "\t\tcolor: rgb( 10, 88, 126 );\n", "\t\tfont-size:20pt;\n", "    }\n", "    h4 {\n", "\t\tfont-family: Tahoma, sans-serif;\n", "\t\tcolor: rgb( 10, 88, 126 );\n", "\t\tfont-size:18pt;\n", "        margin-top:12px;\n", "        margin-bottom: 3px;\n", "       }\n", "\n", "\tul {\n", "\t\tfont-family: Tahoma, sans-serif;\n", "\t\tcolor: rgb( 90, 90, 90 );\n", "\t\tfont-size:11pt;\n", "\t\tline-height: 185%;\n", "\t\t}\n", "\n", "\tp {\n", "\t\tfont-family: Tahoma, sans-serif;\n", "\t\tcolor: rgb( 90, 90, 90 );\n", "\t\tfont-size:11pt;\n", "\t\t}\n", "\n", "\tstrong {\n", "\t\tfont-family: Tahoma, sans-serif;\n", "\t\tcolor: rgb( 30, 30, 30 );\n", "\t\tfont-size:11pt;\n", "\t\t}\n", "\n", "\ta:link {\n", "\t\tfont-family: Tahoma, sans-serif;\n", "\t\tcolor: rgb( 10, 88, 126 );\n", "\t\tfont-size:11pt;\n", "\t\t}\n", "\n", "\ta:visited {\n", "\t\tcolor:rgb( 10, 88, 126 );\n", "\t}\n", "\n", "    div.text_cell_render{\n", "        font-family: Helvetica, Courier, Computer Modern, \"Helvetica Neue\", Arial, Geneva, sans-serif;\n", "\t\tcolor: rgb( 84, 84, 84 );\n", "\t\tfont-size:11pt;\n", "        line-height: 125%;\n", "        font-size: 100%;\n", "        width:800px;\n", "        margin-left:auto;\n", "        margin-right:auto;\n", "    }\n", "    .CodeMirror{\n", "            font-family: Courier, \"Source Code Pro\", source-code-pro,Consolas, monospace;\n", "    }\n", "\n", "    .text_cell_render h5 {\n", "        font-weight: 300;\n", "        font-size: 11pt;\n", "        color: rgb( 48, 48, 48 );\n", "        font-style: italic;\n", "        margin-bottom: .5em;\n", "        margin-top: 0.5em;\n", "        display: block;\n", "    }\n", "\n", "    .warning{\n", "        color: rgb( 240, 20, 20 )\n", "        }\n", ".rendered_html td\n", "{\n", "text-align: right;\n", "}\n", "\n", "</style>\n", "<script>\n", "    MathJax.Hub.Config({\n", "                        TeX: {\n", "                           extensions: [\"AMSmath.js\"]\n", "                           },\n", "                displayAlign: 'center', // Change this to 'center' to center equations.\n", "                \"HTML-CSS\": {\n", "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n", "                }\n", "        });\n", "</script>\n"], "text": ["<IPython.core.display.HTML at 0x10637eed0>"], "prompt_number": 1, "metadata": {}}], "collapsed": false, "prompt_number": 1, "input": ["from addutils import css_notebook\n", "css_notebook()"], "metadata": {}}, {"source": "1 What is Machine Learning ?", "cell_type": "heading", "metadata": {}, "level": 2}, {"source": ["Today we see an explosion of applications that are wide and connected with an emphasis on storage and processing. *Most companies are storing a lot of data but not solving the problem of what to do with it*. Yet most of the information is stored in raw form: There a huge amound of information locked-up in databases: information that is potentially important but has not yet been discovered. The objective of these tutorials is to show the foundamental techniques to **Discover Meaningful Information in Data**.\n", "\n", "**Data Mining** is the extraction of implicit, previously unknown and potentially useful information from data.\n", "\n", "**Machine Learning (ML)** and **Deep Learning (DL)** provide the technical basis of Data Mining. **ML** is about building programs with **tunable parameters** (typically an array of floating point values) that are adjusted automatically so as to improve their behavior by **adapting to previously seen data.**\n", "\n", "**DL** is about modeling high-level abstractions in data by using model architectures composed of **multiple non-linear transformations.**\n", "\n", "**ML and DL** can be considered a subfield of **Artificial Intelligence (AI)** since those algorithms can be seen as building blocks to make computers learn to behave more intelligently by somehow **generalizing** rather that just storing and retrieving data items like a database system would do.\n", "\n", "Most of the examples of these tutorials are taken from the [scikit-learn documentation](http://scikit-learn.org/stable/index.html): check the original documentation for further information.\n", "\n", "If you are a MATLAB<sup>&reg;</sup> user we recommend to read [Numpy for MATLAB Users](http://www.scipy.org/NumPy_for_Matlab_Users) and [Python vs Matlab](http://www.pyzo.org/python_vs_matlab.html).\n", "\n", "For an idea of the **Open Source Approach to science**, we suggest the [Science Code Manifesto](http://sciencecodemanifesto.org/)"], "cell_type": "markdown", "metadata": {}}, {"source": "1.1 Documentation and reference:", "cell_type": "heading", "metadata": {}, "level": 3}, {"source": ["* [Numpy Reference guide](http://docs.scipy.org/doc/numpy/reference/)\n", "* [SciPy Reference](http://docs.scipy.org/doc/scipy/reference/)\n", "* [scikit-learn User Guide](http://scikit-learn.org/stable/user_guide.html)\n", "* [scikit-learn External Resources](http://scikit-learn.org/stable/presentations.html)"], "cell_type": "markdown", "metadata": {}}, {"source": "2 Supervised and Unsupervised Learning", "cell_type": "heading", "metadata": {}, "level": 2}, {"source": ["In general, a learning problem uses a set of n data samples to predict properties of unknown data. Usually data are organized in tables where rows (first axis) represent the **samples** (or **instances**) and colums represent **attributes** (or **features**), for Supervised Learning, another array of **classes** or **target variables** is provided.\n", "\n", "We can separate learning problems in a few large categories:\n", "\n", "In **<font color=\"Green\">SUPERVISED LEARNING</font>**, we have a dataset consisting of both features and labels. The task is to construct an estimator which is able to predict the label of an object given the set of features.\n", "\n", "* We have a **CLASSIFICATION** task when the **target variable is nominal (discrete)**  - examples:\n", "\n", "  * predicting the species of iris given a set of measurements of its flower\n", "  * given a multicolor image of an object through a telescope, determine whether that object is a star, a quasar, or a galaxy.\n", "\n", "* We have a **REGRESSION** task when the **target variable is continuous** - examples:\n", "\n", "  * given a set of attributes, determine the selling price of an house\n", "\n", "\n", "In **<font color=\"Green\">UNSUPERVISED LEARNING</font>** the data has no labels, and we are interested in finding similarities between the samples.\n", "\n", "Unsupervised learning comprises tasks such as *dimensionality reduction*, *clustering*, and *density estimation*.\n", "Some unsupervised learning problems are:\n", "\n", "* **CLUSTERING** is the task that **group similar items together**  - examples:\n", "\n", "  * given observations of distant galaxies, determine which features are most important in distinguishing between them.\n", "\n", "* **DENSITY ESTIMATION** is a task were we want to **find statistical values that describe the data**\n", "\n", "* **DIMENSIONALITY REDUCTION** is for **reduce the number of the features while keeping most of the information**\n", "\n", "**<font color=\"Green\">UNSUPERVISED / SUPERVISED LEARNING</font>** in DL usally the two approach are combined, in fact the DL layers (Restricted Boltzmann Machines, Autoencoders, Convolutional Neural Networks) are used to learn the most significative features of the data. Those features are then used with standard ML regressors or classificators."], "cell_type": "markdown", "metadata": {}}, {"source": "3 Cheat Sheet - scikit-learn Algorithm Selection", "cell_type": "heading", "metadata": {}, "level": 2}, {"source": ["ML is a general concept involving a huge number of algorithms. This is a tentative Cheat Sheet to help finding the correct approach.\n", "\n", "Basically, the principle is to start simple first. If this doesn't work out, try something more complicated.\n", "\n", "<font color=\"Red\">Red Links</font> point to algorithms NOT included in scikit-learn.\n", "\n", "To make any of the algorithms actually work, you need to do the right preprocessing.\n", "\n", "Generally every ML algorithm needs a minimum number of samples. All the methods listed below are applicable to datasets with at least 50 samples. For tasks involving less than 50 samples most of the following methods are not suitable:"], "cell_type": "markdown", "metadata": {}}, {"source": ["* **<font color=\"Green\">To predict a QUANTITY:</font>**\n", "\n", "    * **Regression:** these methods give back a numerical outcome.\n", "\n", "        * **LESS than 100k samples with all features important (dense data):**\n", "\n", "            * TRY: [Ridge Regression](http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression) *(see Generalized Linear Models)*\n", "\n", "                * If Ridge Regression doesn't work, TRY: [Support Vector Regression (svm.SVR)](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) with *linear kernel* *(see Support Vector Machines)*\n", "\n", "                    * If SVR with *linear kernel* doesn't work, TRY: [Support Vector Regression (svm.SVR)](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) with *rbf kernel* *(see Support Vector Machines)*\n", "                        * If none of the above methods work, USE: [Ensemble Regressors](http://scikit-learn.org/stable/modules/ensemble.html) *(RF, Extremely Randomized Trees, GBRT)* *(see Support Ensemble Methods)*\n", "\n", "        * **LESS than 100k samples with few features important (sparse data):**\n", "\n", "            * USE: [Elastic Net Lasso](http://scikit-learn.org/stable/modules/linear_model.html#elastic-net) *(see Generalized Linear Models)*\n", "\n", "        * **MORE than 100k samples:**\n", "            * USE: [SGD Regressor](http://scikit-learn.org/stable/modules/sgd.html#regression) *(see Stochastic Gradient Descent)*\n", "\n", "        * **Alternatively, for every problem size:**\n", "\n", "            * CALL US<br><br>\n", "\n", "    * **Dimensionality Reduction (NOT for predicting the structure of the data):** these methods are suitable for data visualization and human interpretation.\n", "\n", "        * TRY: [RandomizedPCA](http://scikit-learn.org/stable/modules/decomposition.html#principal-component-analysis-pca)\n", "\n", "            * If RandomizedPCA dont work, and you have *LESS than 10k samples*, USE: [t-distributed Stochastic Neighbor Embedding (t-SNE)](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE)\n", "\n", "            * If RandomizedPCA dont work, and you have *MORE than 10k samples*, CALL US: most probably you need a more efficient version of t-SNE\n", "\n", "    * **For Prediction of multivariate or structured outputs:**\n", "\n", "        * TRY: [<font color=\"Red\">SVM struct</font>](http://www.cs.cornell.edu/people/tj/svm_light/svm_struct.html). This algorithm is free for non-commercial use\n", "\n", "        * TRY: [<font color=\"Red\">pystruct</font>](https://github.com/amueller/pystruct). (Under development)"], "cell_type": "markdown", "metadata": {}}, {"source": ["* **<font color=\"Green\">Predict a CATEGORY for LABELED Data (Classification):</font>**\n", "\n", "    * **LESS than 100k samples**, TRY: [Linear SVC](http://scikit-learn.org/stable/modules/svm.html#svc)\n", "\n", "        * If Linear SVC dont work, and you have *NUMERICAL DATA*, TRY: [KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n", "\n", "            * If KNeighborsClassifier doesn't work, USE: [SVC](http://scikit-learn.org/stable/modules/svm.html#svc)\n", "\n", "                * If SVC doesn't work, USE: [Ensemble Classifiers](http://scikit-learn.org/stable/modules/ensemble.html) *(RF, Extremely Randomized Trees, GBRT)*\n", "\n", "        * If Linear SVC dont work, and you have *TEXTUAL DATA*, USE: [Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html)\n", "\n", "    * **MORE than 100k samples**, TRY: [SGD Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)\n", "\n", "        * If SGD Classifier dont work, , USE: [Kernel Approximation](http://scikit-learn.org/stable/modules/kernel_approximation.html)"], "cell_type": "markdown", "metadata": {}}, {"source": ["* **<font color=\"Green\">Predict a CATEGORY for UNLABELED Data (Clustering):</font>**\n", "\n", "    * **LESS than 10k samples and KNOWN number of categories**\n", "        * USE: [Mini Batch K-Means](http://scikit-learn.org/stable/modules/clustering.html#mini-batch-k-means)\n", "\n", "    * **MORE than 10k samples and KNOWN number of categories**\n", "        * TRY: [K-Means Clustering](http://scikit-learn.org/stable/tutorial/statistical_inference/unsupervised_learning.html#k-means-clustering)\n", "\n", "            * If K-Means Clustering doesn't work, TRY: [Gaussian Mixture Models](http://scikit-learn.org/stable/modules/mixture.html#gmm-classifier)\n", "\n", "                * If GMM doesn't work, USE: [Spectral Clustering](http://scikit-learn.org/stable/modules/clustering.html#spectral-clustering)\n", "\n", "    * **LESS than 10k samples and UNKNOWN number of categories**\n", "        * TRY: [Mean Shift](http://scikit-learn.org/stable/modules/clustering.html#mean-shift)\n", "\n", "            * If Mean Shift doesn't work, USE: [Variational Gaussian Mixtures](http://scikit-learn.org/stable/modules/mixture.html#vbgmm-classifier-variational-gaussian-mixtures)"], "cell_type": "markdown", "metadata": {}}, {"source": "4 Machine Learning Wisdom", "cell_type": "heading", "metadata": {}, "level": 2}, {"source": ["Here is a list of things to take in great consideration while developing ML systems:\n", "\n", "1. **No Free Lunch:** A wide variety of techniques exist for modeling. An important theorem in statistical machine learning essentially states that no one technique will outperform all other techniques on all problems *(Wolpert & MacReady, 1997)*. This theorem is sometimes referred to as *No Free Lunch*. Often, a modeling group will specialize in one particular technique, and will tout that technique as the being intrinsically superior to others. Such a claim should be regarded with extreme suspicion. Furthermore, the field of statistical machine learning is evolving rapidly, and new algorithms are developed at a regular pace, this determines a very fast aging for ML approaches. This is the reason why in Addfor we rely on Open Source, Lean and Data-Driven Development and **Combinatorial Innovation**.\n", "\n", "2. **Beware of False Predictors:** In selecting input variables for a model, one must be careful not to include false predictors. A false predictor is a variable that is strongly correlated with the output class, but that is not available in a realistic prediction scenario. This step is stricktly data-dependent and can be accomplished by paying attention to the choice of the validation dataset. **Correlation does not imply causation:** ice-cream sales is a strong predictor for drowning deaths.\n", "\n", "3. **Mind Data Balancing:** Always check if your algorithm is suitable to handle Data Asymmetricity.\n", "\n", "4. **Correctly Define Output Classes:**  If the model's task is to predict a system failure, it seems natural for the output classes to be \"fail\" and \"not fail\". However, characterizing the exact conditions under which failure occurs is not straightforward. For example two failures for different reasons could represent very different classes.\n", "\n", "5. **Data Preparation:** this is maybe the most important task in any predictive algorithm, we dedicate a whole notebook to it! \n", "\n", "6. **Model Selection:** Any modeling technique can be used to construct of a continuum of models, from simple to complex. One of the key issues in modeling is model selection, which involves picking the appropriate level of complexity for a model given a data set. Although model selection methods can be automated to some degree, model selection cannot be avoided. If someone claims otherwise, or does not emphasize their expertise in model selection, one should be suspicious of his abilities.\n", "\n", "7. **Segmentation:** Often, a data set can be broken into several smaller, more homogenous data sets, which is referred to as segmentation. For example, a customer data base might be split into business and residential customers. Although domain experts can readily propose segmentations, enforcing a segmentation suggested by domain experts is generally not the most prudent approach to modeling, because the data itself provides clues to how the segmentation should be performed. Consequently, one should be concerned if a modeler claims to utilize a priori segmentation.\n", "\n", "8. **Model Evaluation:** Once a model has been built, the natural question to ask is how accurate it is. Here we describe common sorts of deception that can occur in assessing and evaluating a model:\n", "\n", "    a) *Failing to use an independent test set:* To obtain a fair estimate of performance, the model must be evaluated on examples that were not contained in the training set. The available data must be split into nonoverlapping subsets, with the test set reserved only for evaluation.\n", "\n", "    b) *Assuming stationarity of the test environment:* For many difficult problems, a model built based on historical data will become a poorer and poorer predictor as time goes on, because the environment is nonstationary--the rules and behaviors of individuals change over time. Consequently, the best measure of a model's true performance will be obtained if it is tested on data from a different point in time relative to the training data.\n", "\n", "    c) *Incomplete reports of results:* An accurate model will correctly discriminate examples of one output class from examples of another output class. Discrimination performance is best reported with an ROC curve, a lift curve, or a precision-recall curve. Any report of accuracy using only a single number is suspect.\n", "\n", "    d) *Filtering data to bias results:* In a large data set, one segment of the population may be easier to predict than another. If a model is trained and tested just on this segment of the population, it will be more accurate than a model that must handle the entire population. Selective filtering can turn a hard problem into an easier problem.\n", "\n", "    e) *Selective sampling of test cases:* A fair evaluation of a model will utilize a test set that is drawn from the same population as the model will eventually encounter in actual usage.\n", "\n", "    f) *Failing to assess statistical reliability:* When comparing the accuracy of two models, it is not sufficient to report that one model performed better than the other, because the difference might not be statistically reliable. \"Statistical reliability\" means, among other things, that if the comparison were repeated using a different sample of the population, the same result would be achieved."], "cell_type": "markdown", "metadata": {}}, {"source": ["---\n", "\n", "Email **training [at ]add-for [dot] com** for more information and custom training programs."], "cell_type": "markdown", "metadata": {}}], "metadata": {}}], "metadata": {"name": "", "signature": "sha256:88dcfd47f65a432b3e655139be5d0457069d406a5f0c88af60dc6332ff38456f"}}