{"nbformat_minor": 0, "cells": [{"source": ["# PyTables"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": ["import addutils.toc ; addutils.toc.js(ipy_notebook=True)"], "outputs": [{"execution_count": 1, "output_type": "execute_result", "data": {"application/javascript": ["\n", "$(function() {\n", "    function regenTOC(){\n", "\tvar toc = document.createElement(\"div\");\n", "\t$(toc).attr(\"class\", \"table-of-contents\");\n", "\n", "\tvar curLevel = 0;\n", "\tvar containerStack = [toc];\n", "\tvar levelOfTag = {\"h2\": 1, \"h3\": 2, \"h4\": 3, \"h5\": 4};\n", "\n", "\tfunction pushLevel() {\n", "            var list = document.createElement(\"ul\");\n", "            containerStack.push(list);\n", "            curLevel++;\n", "\t}\n", "\t\n", "\tfunction popLevel() {\n", "            var lastContainer = containerStack.pop();\n", "            $(lastContainer).appendTo(containerStack[containerStack.length - 1]);\n", "            curLevel--;\n", "\t}\n", "\t\n", "\t$(\".text_cell_render :header\").each(function (i, elem) {\n", "            var level = levelOfTag[ elem.tagName.toLowerCase() ];\n", "\n", "            if (level === undefined)\n", "\t\treturn;\n", "\n", "            while (curLevel < level)\n", "\t\tpushLevel();\n", "            while (curLevel > level)\n", "\t\tpopLevel();\n", "            \n", "            var listItem = document.createElement(\"li\");\n", "            var link = document.createElement(\"a\");\n", "            $(link)\n", "\t\t.text($(elem).contents().first().text()) // Remove the pilcrow sign\n", "\t\t.attr(\"href\", \"#\" + $(elem).attr(\"id\"))\n", "\t\t.appendTo(listItem);\n", "            $(listItem).appendTo(containerStack[containerStack.length - 1]);\n", "\t});\n", "\t\n", "\twhile (curLevel > 0)\n", "            popLevel();\n", "\n", "        $(\"<a class='btn-update' href='#'>Aggiorna</a>\")\n", "          .click(regenTOC).prependTo(toc);\n", "\n", "\t$(toc).prepend(\"<div class='title'>Contents</div>\")\n", "          .wrap(\"<div class='toc-container'/>\");\n", "\n", "        $(element).empty();\n", "        $(element).append(toc);\n", "    }\n", "\n", "    $([IPython.events]).on('notebook_loaded.Notebook', regenTOC);\n", "    regenTOC();\n", "});\n", "\n"], "text/plain": ["<IPython.core.display.Javascript object>"]}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": ["PyTables is an high-performance, on-disk data container, query engine and computation kernel, with an easy-to-use interface, developed by Francesc Alted since 2002."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": ["from addutils import css_notebook\n", "css_notebook()"], "outputs": [{"execution_count": 2, "output_type": "execute_result", "data": {"text/plain": ["<IPython.core.display.HTML object>"], "text/html": ["<style>\n", ".text_cell_render @font-face {\n", "    font-family: \"Computer Modern\";\n", "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n", "}\n", "\n", "div.cell {\n", "    width: 900px;\n", "    margin-left: 0% !important;\n", "    margin-right: 0%;\n", "}\n", "\n", "code {\n", "    font-size:10pt;\n", "}\n", "\n", ".text_cell_render  h1 {\n", "    font-family: Tahoma, sans-serif;\n", "    color: rgb( 10, 88, 126 );\n", "    font-size:28pt;\n", "}\n", ".text_cell_render h2 {\n", "    font-family: Tahoma, sans-serif;\n", "    color: rgb( 10, 88, 126 );\n", "    font-size:24pt;\n", "}\n", ".text_cell_render h3 {\n", "    font-family: Tahoma, sans-serif;\n", "    color: rgb( 10, 88, 126 );\n", "    font-size:20pt;\n", "}\n", ".text_cell_render h4 {\n", "    font-family: Tahoma, sans-serif;\n", "    color: rgb( 10, 88, 126 );\n", "    font-size:18pt;\n", "    margin-top:12px;\n", "    margin-bottom: 3px;\n", "}\n", "\n", ".text_cell_render h5 {\n", "    font-weight: 300;\n", "    font-size: 11pt;\n", "    color: rgb( 48, 48, 48 );\n", "    font-style: italic;\n", "    margin-bottom: .5em;\n", "    margin-top: 0.5em;\n", "    display: block;\n", "}\n", "\n", ".text_cell_render ul {\n", "    font-family: Tahoma, sans-serif;\n", "    color: rgb( 90, 90, 90 );\n", "    font-size:11pt;\n", "    line-height: 185%;\n", "}\n", "\n", ".text_cell_render yp {\n", "    font-family: Tahoma, sans-serif;\n", "    color: rgb( 90, 90, 90 );\n", "    font-size:11pt;\n", "}\n", "\n", ".text_cell_render strong {\n", "    font-family: Tahoma, sans-serif;\n", "    color: rgb( 30, 30, 30 );\n", "    font-size:11pt;\n", "}\n", "\n", ".text_cell_render a:link {\n", "    font-family: Tahoma, sans-serif;\n", "    color: rgb( 10, 88, 126 );\n", "    font-size:11pt;\n", "}\n", "\n", ".text_cell_render a:visited {\n", "    color:rgb( 10, 88, 126 );\n", "}\n", "\n", ".text_cell_render {\n", "    font-family: Helvetica, Courier, Computer Modern, \"Helvetica Neue\", Arial, Geneva, sans-serif;\n", "    color: rgb( 84, 84, 84 );\n", "    font-size:11pt;\n", "    line-height: 125%;\n", "    font-size: 100%;\n", "    width:800px;\n", "}\n", "\n", ".CodeMirror {\n", "    font-family: Courier, \"Source Code Pro\", source-code-pro,Consolas, monospace;\n", "}\n", "\n", ".warning {\n", "    color: rgb( 240, 20, 20 );\n", "}\n", "\n", "/* Pandas tables */\n", "/*\n", ".rendered_html td {\n", "    text-align: right;\n", "}\n", "*/\n", "\n", "table.dataframe td {\n", "    text-align: right;\n", "}\n", "\n", ".output .table-of-contents {\n", "    border: 1px #cecece solid;\n", "    background-color: #fafafa;\n", "    padding-top: 10px;\n", "    padding-bottom: 5px;\n", "    padding-right: 15px;\n", "    padding-left: 0px;\n", "    margin-bottom: 20px;\n", "    display: inline-block;\n", "    position: relative;\n", "}\n", "\n", ".output .table-of-contents ul {\n", "    list-style-type: none;\n", "    padding-left: 20px;\n", "}\n", "\n", ".output .table-of-contents .title {\n", "    font-weight: bold;\n", "    font-height: 11pt;\n", "    padding-left: 20px; /* looks better if it's the same to the <ul> */\n", "}\n", "\n", ".output .table-of-contents .btn-update {\n", "    position: absolute;\n", "    float: right;\n", "    right: 11px;\n", "    top: 4px;\n", "    font-size: 9pt;\n", "}\n", "\n", "</style>\n", "<script>\n", "    MathJax.Hub.Config({\n", "                        TeX: {\n", "                           extensions: [\"AMSmath.js\"]\n", "                           },\n", "                displayAlign: 'center', // Change this to 'center' to center equations.\n", "                \"HTML-CSS\": {\n", "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n", "                }\n", "        });\n", "</script>\n"]}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": ["**PyTables - What is it?**\n", "\n", "PyTables is a Python package wich allows dealing with HDF5 tables and:\n", "\n", "* a binary data container for on-disk, structured data\n", "* with support for data compression: Zlib, bzip2, LZO and Blosc\n", "* with powerful query and indexing capabilities\n", "* can perform out-of-core (data on-disk) operations very efficiently\n", "* based on the standard de-facto [HDF5](http://www.hdfgroup.org/HDF5/) format\n", "* free software ([BSD license](http://opensource.org/licenses/BSD-2-Clause))"], "cell_type": "markdown", "metadata": {}}, {"source": ["**PyTables - What is not**\n", "\n", "* NOT a relational database replacement\n", "* not a distributed database\n", "* not extremely secure of safe (it's more about speed)\n", "* not a mere HDF5  wrapper"], "cell_type": "markdown", "metadata": {}}, {"source": ["## 1 The Array Object"], "cell_type": "markdown", "metadata": {}}, {"source": ["---\n", "\n", "<a name=\"arrayobject\"><img src=\"files/utilities/arrayobject.jpg\"  style=\"max-width: 100%\"/>"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": ["import numpy as np\n", "import tables as tb"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 4, "cell_type": "code", "source": ["f = tb.openFile('temp/atest.h5', 'w')  # Create a new file in \"write\" mode\n", "a = np.arange(50).reshape(5,10)        # Create a NumPy array\n", "f.createArray(f.root, 'my_array1', a)     # Save the array"], "outputs": [{"execution_count": 4, "output_type": "execute_result", "data": {"text/plain": ["/my_array1 (Array(5, 10)) ''\n", "  atom := Int64Atom(shape=(), dflt=0)\n", "  maindim := 0\n", "  flavor := 'numpy'\n", "  byteorder := 'little'\n", "  chunkshape := None"]}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": ["The `createArray` method returns a handler of the array **on disk**. This handler reports that we are working with an array called `my_array1` made of 32 bit integers. 'Flavor' indicates that this array has been created by numpy.\n", "\n", "Now we can retrieve the data from disk by using the indexing notation:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": ["f.root.my_array1[:]"], "outputs": [{"execution_count": 5, "output_type": "execute_result", "data": {"text/plain": ["array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n", "       [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n", "       [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n", "       [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n", "       [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]])"]}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": ["We can also select sub-arrays. In this case, just the data related to the sub-array are actually read from disk:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": ["f.root.my_array1[1:5:2,0:5]"], "outputs": [{"execution_count": 6, "output_type": "execute_result", "data": {"text/plain": ["array([[10, 11, 12, 13, 14],\n", "       [30, 31, 32, 33, 34]])"]}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": ["Using `np.allclose` we can check that the data read from disk are equal to the corresponding data in RAM:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": ["np.allclose(f.root.my_array1[1:5:2,0:5], a[1:5:2,0:5])"], "outputs": [{"execution_count": 7, "output_type": "execute_result", "data": {"text/plain": ["True"]}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": ["HDF5 files have a hierarchical structure. We have now a `atest.h5` file that contains an Array named `'my_array1'`. We can create a second array in the same file called `'my_array2'`:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": ["f.createArray(f.root, 'my_array2', np.arange(10))\n", "f.root"], "outputs": [{"execution_count": 8, "output_type": "execute_result", "data": {"text/plain": ["/ (RootGroup) u''\n", "  children := ['my_array1' (Array), 'my_array2' (Array)]"]}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": ["... And we can attach to these arrays additional information in form of attributes (metadata):"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": ["f.root.my_array1.attrs"], "outputs": [{"execution_count": 9, "output_type": "execute_result", "data": {"text/plain": ["/my_array1._v_attrs (AttributeSet), 4 attributes:\n", "   [CLASS := 'ARRAY',\n", "    FLAVOR := 'numpy',\n", "    TITLE := '',\n", "    VERSION := '2.4']"]}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 10, "cell_type": "code", "source": ["f.root.my_array1.attrs.MY_ATTRIBUTE = \"This is a metadata I can attach to any array\"\n", "f.root.my_array1.attrs"], "outputs": [{"execution_count": 10, "output_type": "execute_result", "data": {"text/plain": ["/my_array1._v_attrs (AttributeSet), 5 attributes:\n", "   [CLASS := 'ARRAY',\n", "    FLAVOR := 'numpy',\n", "    MY_ATTRIBUTE := 'This is a metadata I can attach to any array',\n", "    TITLE := '',\n", "    VERSION := '2.4']"]}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": ["Now check the `temp/atest.h5` filesize: it's zero! This is because PyTables is highly optimized and keeps the data in RAM (bufferize IO) until the file is closed or there is not available RAM or when you explicitly call a `flush()` method."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": ["# Flush data to the file (very important to keep all your data safe!)\n", "f.flush()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Check again the `atest.h5` filesize: now the data has been flushed and the file got a size different from zero."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": ["f.close()  # close access to file"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["## 2 The CArray Object"], "cell_type": "markdown", "metadata": {}}, {"source": ["---\n", "\n", "<a name=\"carrayobject\"><img src=\"files/utilities/carrayobject.jpg\"  style=\"max-width: 100%\"/>"], "cell_type": "markdown", "metadata": {}}, {"source": ["When creating a new CArray (Compressible Array), type and shape must be passed to the constructor:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 13, "cell_type": "code", "source": ["f = tb.openFile('temp/ctest.h5', 'w')\n", "f.createCArray(f.root, 'my_carray1', tb.Float64Atom(), (10000,1000))"], "outputs": [{"execution_count": 13, "output_type": "execute_result", "data": {"text/plain": ["/my_carray1 (CArray(10000, 1000)) ''\n", "  atom := Float64Atom(shape=(), dflt=0.0)\n", "  maindim := 0\n", "  flavor := 'numpy'\n", "  byteorder := 'little'\n", "  chunkshape := (16, 1000)"]}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 14, "cell_type": "code", "source": ["f.flush()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Now check the `temp/ctest.h5` filesize: it's 1KB even if the array is Float64 10000x1000. This is because PyTables just stored the CArray metadata. Now we'll push some data in the CArray container. For simplicity we define a new name for the carray handle: `ca = f.root.my_carray1`"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 15, "cell_type": "code", "source": ["ca = f.root.my_carray1\n", "na = np.linspace(0, 1, 1e7).reshape(10000,1000)\n", "%time ca[:] = na"], "outputs": [{"output_type": "stream", "name": "stdout", "text": ["CPU times: user 25.4 ms, sys: 53.4 ms, total: 78.8 ms\n", "Wall time: 1.42 s\n"]}], "metadata": {"collapsed": false}}, {"execution_count": 16, "cell_type": "code", "source": ["f.close()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Now check the `temp/catest.h5` filesize: it's 76MB whis is exactly the uncompressed space required by a 64bit 10000x1000 matrix.\n", "\n", "CArray allows for data compression: lets try to use a `blosc` compressor with `complevel=5`: filesize must be reduced to 8.7MB!"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 17, "cell_type": "code", "source": ["f = tb.openFile('temp/ctest.h5', 'w')\n", "f.createCArray(f.root, 'my_carray1', tb.Float64Atom(), (10000,1000),\n", "               filters=tb.Filters(complevel=5, complib='blosc'))\n", "%time f.root.my_carray1[:] = na\n", "f.close()"], "outputs": [{"output_type": "stream", "name": "stdout", "text": ["CPU times: user 115 ms, sys: 33.8 ms, total: 149 ms\n", "Wall time: 149 ms\n"]}], "metadata": {"collapsed": false}}, {"source": ["***Try by yourself*** the following compression options:\n", "    \n", "    (complevel=9, complib='blosc')\n", "    (complevel=5, complib='zlib')\n", "    (complevel=9, complib='lzo')\n", "    (complevel=5, complib='bzip2')"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["We can now check how much do it takes to read a small, non-contiguous sub-array by using the `%timeit` magic function: the read operation will be run multiple times to better measure the execution time:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 18, "cell_type": "code", "source": ["f = tb.openFile('temp/ctest.h5', 'r')\n", "%timeit f.root.my_carray1[:4,::100]\n", "f.close()"], "outputs": [{"output_type": "stream", "name": "stdout", "text": ["The slowest run took 141.10 times longer than the fastest. This could mean that an intermediate result is being cached \n", "10000 loops, best of 3: 88.2 \u00b5s per loop\n"]}], "metadata": {"collapsed": false}}, {"source": ["## 3 The Table Object"], "cell_type": "markdown", "metadata": {}}, {"source": ["---\n", "<a name=\"tableobject\"><img src=\"files/utilities/tableobject.jpg\"  style=\"max-width: 100%\"/>"], "cell_type": "markdown", "metadata": {}}, {"source": ["To create a new Table Object we must first describe the fields:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 19, "cell_type": "code", "source": ["class TabularData(tb.IsDescription):\n", "    col1 = tb.StringCol(200)\n", "    col2 = tb.IntCol()\n", "    col3 = tb.Float32Col(10)"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 20, "cell_type": "code", "source": ["# Open a file and create the Table container\n", "f = tb.openFile('temp/atable.h5', 'w')\n", "t = f.createTable(f.root, 'my_table1', TabularData, 'Table Title',\n", "                  filters=tb.Filters(complevel=5, complib='blosc'))"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 21, "cell_type": "code", "source": ["t"], "outputs": [{"execution_count": 21, "output_type": "execute_result", "data": {"text/plain": ["/my_table1 (Table(0,), shuffle, blosc(5)) 'Table Title'\n", "  description := {\n", "  \"col1\": StringCol(itemsize=200, shape=(), dflt='', pos=0),\n", "  \"col2\": Int32Col(shape=(), dflt=0, pos=1),\n", "  \"col3\": Float32Col(shape=(10,), dflt=0.0, pos=2)}\n", "  byteorder := 'little'\n", "  chunkshape := (268,)"]}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 22, "cell_type": "code", "source": ["#  Fill the table with some 1 million rows\n", "from time import time\n", "t0 = time()\n", "r = t.row\n", "for i in xrange(1000*1000):\n", "    r['col1'] = str(i)\n", "    r['col2'] = i+1\n", "    r['col3'] = np.arange(10, dtype=np.float32)+i\n", "    r.append()\n", "t.flush()\n", "print \"Insert time: {:.3f}s\".format(time()-t0,)"], "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Insert time: 11.035s\n"]}], "metadata": {"collapsed": false}}, {"execution_count": 23, "cell_type": "code", "source": ["t"], "outputs": [{"execution_count": 23, "output_type": "execute_result", "data": {"text/plain": ["/my_table1 (Table(1000000,), shuffle, blosc(5)) 'Table Title'\n", "  description := {\n", "  \"col1\": StringCol(itemsize=200, shape=(), dflt='', pos=0),\n", "  \"col2\": Int32Col(shape=(), dflt=0, pos=1),\n", "  \"col3\": Float32Col(shape=(10,), dflt=0.0, pos=2)}\n", "  byteorder := 'little'\n", "  chunkshape := (268,)"]}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": ["`chunkshape := (268,)` means that every 268 rows a datachunk is created, compressed and saved on disk.\n", "\n", "The uncompressed size of the dataset can be calculated by multiplying the number of rows `t.shape[0]` by the size of each row `t.dtype.itemsize`. In this example the size of each row is 244 bytes: 200 bytes for the string field, 4 for the Int field and 40 for the ten Float32. If you check the filesize on disk you will see that since we used a blosc compression algorithm, we managed to reduce the **filesize from 232MB to 3.6MB**!"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 24, "cell_type": "code", "source": ["t.shape[0]*t.dtype.itemsize/2**20."], "outputs": [{"execution_count": 24, "output_type": "execute_result", "data": {"text/plain": ["232.696533203125"]}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": ["With Tables we can do queries. For example here we extract values of `col1` where `col2 < 10`: in **less than one second we query 1.000.000 records**."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 25, "cell_type": "code", "source": ["%time [r['col1'] for r in t if r['col2'] < 10]"], "outputs": [{"output_type": "stream", "name": "stdout", "text": ["CPU times: user 359 ms, sys: 6.01 ms, total: 365 ms\n", "Wall time: 380 ms\n"]}, {"execution_count": 25, "output_type": "execute_result", "data": {"text/plain": ["['0', '1', '2', '3', '4', '5', '6', '7', '8']"]}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": ["We can be even faster by using **in-kernel methods** instead of using the regular Python condition. Condition defined as a string with the `where` method are evaluated. [numexpr](http://code.google.com/p/numexpr/) is a package that accepts the expression as a string, analyzes it, rewrites it more efficiently, and compiles it on the fly into code suitable to its internal virtual machine (VM). Due to its integrated just-in-time (JIT) compiler, it does not require a compiler at runtime:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 26, "cell_type": "code", "source": ["# Repeat the query, but using in-kernel method\n", "%time [r['col1'] for r in t.where('col2 < 10')]"], "outputs": [{"output_type": "stream", "name": "stdout", "text": ["CPU times: user 287 ms, sys: 31.6 ms, total: 318 ms\n", "Wall time: 318 ms\n"]}, {"execution_count": 26, "output_type": "execute_result", "data": {"text/plain": ["['0', '1', '2', '3', '4', '5', '6', '7', '8']"]}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": ["Alternatively, to reach even greater performances, the Table Object support indexing for every column. We can index the colum two:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 27, "cell_type": "code", "source": ["%time t.cols.col2.createCSIndex()"], "outputs": [{"output_type": "stream", "name": "stdout", "text": ["CPU times: user 644 ms, sys: 32.1 ms, total: 676 ms\n", "Wall time: 959 ms\n"]}, {"execution_count": 27, "output_type": "execute_result", "data": {"text/plain": ["1000000"]}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": ["From now on, any query involving col2 will be sped-up many times. In this case we query the whole 1.000.000 records in less than 200us (0.0002s):"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 28, "cell_type": "code", "source": ["%timeit [r['col1'] for r in t.where('col2 < 10')]"], "outputs": [{"output_type": "stream", "name": "stdout", "text": ["The slowest run took 26.42 times longer than the fastest. This could mean that an intermediate result is being cached \n", "10000 loops, best of 3: 109 \u00b5s per loop\n"]}], "metadata": {"collapsed": false}}, {"source": ["Queries can involve both indexed and non-indexed colums, in this case the speed-up will be less noticeable. here we do the query in few seconds because `col3` is not indexed:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 29, "cell_type": "code", "source": ["# Performing complex conditions (regular query)\n", "%time [r['col1'] for r in t if r['col2'] > 10 and r['col3'][0] < 20]"], "outputs": [{"output_type": "stream", "name": "stdout", "text": ["CPU times: user 4.84 s, sys: 3.85 ms, total: 4.85 s\n", "Wall time: 4.84 s\n"]}, {"execution_count": 29, "output_type": "execute_result", "data": {"text/plain": ["['10', '11', '12', '13', '14', '15', '16', '17', '18', '19']"]}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 30, "cell_type": "code", "source": ["f.close()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["## 4 The EArray Object"], "cell_type": "markdown", "metadata": {}}, {"source": ["---\n", "\n", "<a name=\"earrayobject\"><img src=\"files/utilities/earrayobject.jpg\"  style=\"max-width: 100%\"/>"], "cell_type": "markdown", "metadata": {}}, {"source": ["We will illustrate how to use the PyTables EArray (Extensible Array) Object by performing an Out-of-Core calculation. PyTables leverages [numexpr](http://code.google.com/p/numexpr/) to perform computations with arrays that are on disk and not in memory (Out-of-Core). `Numexpr` performs the computation on large arrays by splitting the arrays in smaller blocks of data, those blocks are then uploaded to the CPU cache memory and the computations are done without macking data copies on RAM"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 31, "cell_type": "code", "source": ["import numpy as np\n", "import tables as tb"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 32, "cell_type": "code", "source": ["f = tb.openFile('temp/poly1.h5', 'w')"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Create an empty EArray then populate it with 10 chuncks of 1.000.000 values, we'll have an array with 1 Column and 10.000.000 Rows. In the `createEArray` method we define the dimension along which the EArray can be expanded. For example, in this case whe define by using `(0,)` that the array will be expanded along the row dimension. Similarly, if we wanted to expand it by columns, we would use `(,0)`."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 33, "cell_type": "code", "source": ["ea = f.createEArray(f.root, 'my_earray1', tb.Float64Atom(), (0,),\n", "                    filters=tb.Filters(complevel=5, complib='blosc'))\n", "for s in range(10):\n", "    ea.append(np.linspace(s, s+1, 1e6))\n", "ea.flush()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Create the expression to compute: this expression must be defined as a string. Basically, for each row the polynomial will be calculated:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 34, "cell_type": "code", "source": ["expr = tb.Expr('0.25*ea**3 + 0.75*ea**2 + 1.5*ea - 2')"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Now we have to create an array to store the resulting values, in this case we can decide to use a Compressed Array (CArray) with the same lenght of the EArray: 10.000.000 rows"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 35, "cell_type": "code", "source": ["if hasattr(f.root, 'output_values'):\n", "    f.removeNode(f.root.y)\n", "y = f.createCArray(f.root, 'output_values', tb.Float64Atom(), (len(ea),),\n", "                   filters=tb.Filters(complevel=5, complib='blosc'))"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Specify that the ouput of the expression has to go to **y** on disk"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 36, "cell_type": "code", "source": ["expr.setOutput(y)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["On a standard PC this will take less than a second. This time is significant if you think that data are loaded and stored directly on disk while doing calculation:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 37, "cell_type": "code", "source": ["%time expr.eval()"], "outputs": [{"output_type": "stream", "name": "stdout", "text": ["CPU times: user 632 ms, sys: 94.9 ms, total: 727 ms\n", "Wall time: 1.17 s\n"]}, {"execution_count": 37, "output_type": "execute_result", "data": {"text/plain": ["/output_values (CArray(10000000,), shuffle, blosc(5)) ''\n", "  atom := Float64Atom(shape=(), dflt=0.0)\n", "  maindim := 0\n", "  flavor := 'numpy'\n", "  byteorder := 'little'\n", "  chunkshape := (16384,)"]}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 38, "cell_type": "code", "source": ["f.flush()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["---\n", "\n", "<img src=\"/files/resources/logo.png\" > Visit [www.add-for.com](<http://www.add-for.com/IT>) for more tutorials and updates"], "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "2.7.9", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}